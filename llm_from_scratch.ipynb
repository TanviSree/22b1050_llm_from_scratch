{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqdslkIbCF9Q",
        "outputId": "9567f648-2457-4014-8316-b37a36e30a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters is : 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "with open(\"verdict.txt\",\"r\") as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total characters is :\",len(raw_text))\n",
        "print(raw_text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 : Creating the Embeddings from raw text input"
      ],
      "metadata": {
        "id": "5PN02SQREc_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the vocabulary - mapping of all unique tokens(words/special characters) to their token IDs"
      ],
      "metadata": {
        "id": "U8yjwbuPLeOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "result = re.split(r'([.,:;_?!\"()\\']|--|\\s)',raw_text)\n",
        "fin_res = [item for item in result if item.strip()]\n",
        "all_words = sorted(list(set(fin_res)))\n",
        "all_words.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
        "vocab_size = len(all_words)\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "print(\"Vocabulary size : \",vocab_size)\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i==25:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUH2sjIXKCmZ",
        "outputId": "89a5972e-f786-4b4e-c426-2e2c6087bf15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size :  1132\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer class that can encode as well as decode"
      ],
      "metadata": {
        "id": "ZrKMe_iyLkrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tokenizerv1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {v:k for k,v in vocab.items()}\n",
        "  def encode(self,text):\n",
        "    tokens = re.split(r'([.,:;_?!\"()\\']|--|\\s)',text)\n",
        "    fin_res = [item for item in tokens if item.strip()]\n",
        "    fin_res = [item if item in self.str_to_int else \"<|unk|>\" for item in fin_res]\n",
        "    return [self.str_to_int[item] for item in fin_res]\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "P3t9PxsmLj9U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tokenizerv1(vocab)\n",
        "text = \"Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFRr7mtkNS2z",
        "outputId": "96ea8b55-942d-4558-9cee-27031ef2a442"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[108, 0, 6, 399, 1007, 988, 795, 722, 50, 2, 850, 976, 53, 436, 117, 1016, 418, 988, 420, 1108, 395, 7, 80, 57, 38, 0, 93, 1112, 514, 654, 546, 6, 585, 1077, 444, 987, 994, 879, 687, 546, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "id": "dwJT6RfsNvvt",
        "outputId": "2f4c6749-db43-43e2-8b2b-7e905657a991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Well! -- even through the prism of Hermia' s tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him -- it was fitting that they should mourn him.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now testing out tokenizer after adding tokens for unknown and endoftext:"
      ],
      "metadata": {
        "id": "ZrCQqeUKlESs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab))\n",
        "text = \"even through the prism of Hermia's tears I felt Tanvi to face\"\n",
        "print(tokenizer.encode(text))\n",
        "text2 = \"Poor Jack Gisburn! The women had made him\"\n",
        "text3 = \" <|endoftext|> \".join((text,text2))\n",
        "ids = tokenizer.encode(text3)\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSXAWuFVlDsU",
        "outputId": "9e71c23a-f45b-4f7b-c77d-fdfc45d974c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n",
            "[399, 1007, 988, 795, 722, 50, 2, 850, 976, 53, 436, 1130, 1016, 418]\n",
            "even through the prism of Hermia' s tears I felt <|unk|> to face <|endoftext|> Poor Jack Gisburn! The women had made him\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying out the BPE tokenizer"
      ],
      "metadata": {
        "id": "ZZFM91D3qzcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"I felt the happiest at DiagonAlley with Hagrid <|endoftext|>\"\n",
        "ids = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR7mPer-q4E1",
        "outputId": "ef45c5e2-a490-4eee-8bd9-5247e2982075"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 2936, 262, 49414, 379, 6031, 1840, 2348, 1636, 351, 21375, 6058, 220, 50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gK5vgJHXsh7X",
        "outputId": "49df9d37-061d-47e6-f272-9d8350ce85e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I felt the happiest at DiagonAlley with Hagrid <|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}