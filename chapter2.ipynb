{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCJhYoGhRVJ52nj59cEY8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanviSree/22b1050_llm_from_scratch/blob/main/chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqdslkIbCF9Q",
        "outputId": "674db92e-8596-47aa-a52e-7fe50123b13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters is : 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "with open(\"verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total characters is :\",len(raw_text))\n",
        "print(raw_text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 : Creating the Embeddings from raw text input"
      ],
      "metadata": {
        "id": "5PN02SQREc_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the vocabulary - mapping of all unique tokens(words/special characters) to their token IDs"
      ],
      "metadata": {
        "id": "U8yjwbuPLeOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "result = re.split(r'([.,:;_?!\"()\\']|--|\\s)',raw_text)\n",
        "fin_res = [item for item in result if item.strip()]\n",
        "all_words = sorted(list(set(fin_res)))\n",
        "all_words.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
        "vocab_size = len(all_words)\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "print(\"Vocabulary size : \",vocab_size)\n",
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i==25:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUH2sjIXKCmZ",
        "outputId": "60e7829d-497b-4e9f-d28a-6bb1106e4108"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size :  1132\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer class that can encode as well as decode"
      ],
      "metadata": {
        "id": "ZrKMe_iyLkrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tokenizerv1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {v:k for k,v in vocab.items()}\n",
        "  def encode(self,text):\n",
        "    tokens = re.split(r'([.,:;_?!\"()\\']|--|\\s)',text)\n",
        "    fin_res = [item for item in tokens if item.strip()]\n",
        "    fin_res = [item if item in self.str_to_int else \"<|unk|>\" for item in fin_res]\n",
        "    return [self.str_to_int[item] for item in fin_res]\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "P3t9PxsmLj9U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tokenizerv1(vocab)\n",
        "text = \"Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFRr7mtkNS2z",
        "outputId": "2078142e-d6f8-49e6-9636-b88f70622e7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[108, 0, 6, 399, 1007, 988, 795, 722, 50, 2, 850, 976, 53, 436, 117, 1016, 418, 988, 420, 1108, 395, 7, 80, 57, 38, 0, 93, 1112, 514, 654, 546, 6, 585, 1077, 444, 987, 994, 879, 687, 546, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "id": "dwJT6RfsNvvt",
        "outputId": "4debba0b-20f7-4d07-b7fe-25083c5968f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Well! -- even through the prism of Hermia' s tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him -- it was fitting that they should mourn him.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now testing out tokenizer after adding tokens for unknown and endoftext:"
      ],
      "metadata": {
        "id": "ZrCQqeUKlESs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab))\n",
        "text = \"even through the prism of Hermia's tears I felt Tanvi to face\"\n",
        "print(tokenizer.encode(text))\n",
        "text2 = \"Poor Jack Gisburn! The women had made him\"\n",
        "text3 = \" <|endoftext|> \".join((text,text2))\n",
        "ids = tokenizer.encode(text3)\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSXAWuFVlDsU",
        "outputId": "5b0c16c9-51c6-4421-baff-11177af6cc2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n",
            "[399, 1007, 988, 795, 722, 50, 2, 850, 976, 53, 436, 1130, 1016, 418]\n",
            "even through the prism of Hermia' s tears I felt <|unk|> to face <|endoftext|> Poor Jack Gisburn! The women had made him\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying out the BPE tokenizer"
      ],
      "metadata": {
        "id": "ZZFM91D3qzcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"I felt the happiest at DiagonAlley with Hagrid <|endoftext|>\"\n",
        "ids = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR7mPer-q4E1",
        "outputId": "8511dd2f-ada9-417c-d460-aaa29b659364"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 2936, 262, 49414, 379, 6031, 1840, 2348, 1636, 351, 21375, 6058, 220, 50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gK5vgJHXsh7X",
        "outputId": "30e6a07c-22b9-4756-c178-9731f2a627a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I felt the happiest at DiagonAlley with Hagrid <|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating input-target pairs using sliding window approach"
      ],
      "metadata": {
        "id": "K2WSvLObAowj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc1 = tokenizer.encode(raw_text)\n",
        "enc1 = enc1[50:]\n",
        "context_size = 4\n",
        "for i in range(1,context_size):\n",
        "  x = enc1[:i]\n",
        "  y = enc1[i]\n",
        "  print(tokenizer.decode(x),\"=>\",tokenizer.decode([y]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faYC3P25AvQ2",
        "outputId": "5c5fb66d-8756-4fdb-eb27-d112c0bb10b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and =>  established\n",
            " and established =>  himself\n",
            " and established himself =>  in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the pytorch Dataset and DataLoader class to create a custom class"
      ],
      "metadata": {
        "id": "b-2ez1iiTXK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "class GPTDataset(Dataset):\n",
        "  def __init__(self,text,tokenizer,max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "        input_chunk = token_ids[i:i+max_length]\n",
        "        target_chunk = token_ids[i+1:i+max_length+1]\n",
        "        self.input_ids.append(torch.tensor(input_chunk,dtype=torch.long))\n",
        "        self.target_ids.append(torch.tensor(target_chunk,dtype=torch.long))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx],self.target_ids[idx]"
      ],
      "metadata": {
        "id": "cUrlHiyOLnwt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_dataloader(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDataset(txt,tokenizer,max_length,stride)\n",
        "  dataloader = DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "zbaTkY_5R0GX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing this with our verdict short story"
      ],
      "metadata": {
        "id": "xsXrPrCNTf7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = gpt_dataloader(raw_text,batch_size=4,max_length=4,stride=4,shuffle=False,drop_last=True,num_workers=0)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "#basically batch size is number of samples u want to process in the batch, max_length is no.words u want the input sentence to have and stride is offset by which pointer moves to get nex input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQooB_r0TVqJ",
        "outputId": "5d98cbd1-7228-4eba-94c2-492ce3bf0a8d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257]]), tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs,targets= next(data_iter)\n",
        "print(\"Inputs : \",inputs)\n",
        "print(\"Targets : \",targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3WrHaVTUryL",
        "outputId": "08ae8c20-cb0f-47cb-f956-b71c8e4ecd03"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs :  tensor([[  287,   262,  6001,   286],\n",
            "        [  465, 13476,    11,   339],\n",
            "        [  550,  5710,   465, 12036],\n",
            "        [   11,  6405,   257,  5527]])\n",
            "Targets :  tensor([[  262,  6001,   286,   465],\n",
            "        [13476,    11,   339,   550],\n",
            "        [ 5710,   465, 12036,    11],\n",
            "        [ 6405,   257,  5527, 27075]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting token IDs to embeddings"
      ],
      "metadata": {
        "id": "wJSQ5YB7Xr68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for a sample of lets say 20 words in a vocabulary, and output dim of 3, we are creating an embedding matrix\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(20,3)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPKS41KYXret",
        "outputId": "2eeab1c7-35d1-47ff-e78e-776c37a8f5f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.3035],\n",
            "        [-0.5880,  0.3486,  0.6603],\n",
            "        [-0.2196, -0.3792,  0.7671],\n",
            "        [-1.1925,  0.6984, -1.4097],\n",
            "        [ 0.1794,  1.8951,  0.4954],\n",
            "        [ 0.2692, -0.0770, -1.0205],\n",
            "        [-0.1690,  0.9178,  1.5810],\n",
            "        [ 1.3010,  1.2753, -0.2010],\n",
            "        [ 0.4965, -1.5723,  0.9666],\n",
            "        [-1.1481, -1.1589,  0.3255],\n",
            "        [-0.6315, -2.8400, -1.3250],\n",
            "        [ 0.1784, -2.1338,  1.0524],\n",
            "        [-0.3885, -0.9343, -0.4991],\n",
            "        [-1.0867,  0.8805,  1.5542],\n",
            "        [ 0.6266, -0.1755,  1.3111],\n",
            "        [-0.2199,  0.2190,  0.2045],\n",
            "        [ 0.5146,  0.9938, -0.2587],\n",
            "        [-1.0826,  0.1036, -2.1996],\n",
            "        [-0.0885, -0.5612,  0.6716],\n",
            "        [ 0.6933, -0.9487, -0.0765]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding positions into embeddings"
      ],
      "metadata": {
        "id": "cMrtM_BTgiDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just create another embedding matrix with appropriate dimensions and addit to tokens embedding matrix\n",
        "torch.manual_seed(123)\n",
        "max_length = 4\n",
        "token_embedding_layer = torch.nn.Embedding(50257,256)\n",
        "dataloader= gpt_dataloader(raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False,drop_last=True,num_workers=0)\n",
        "data_iter = iter(dataloader)\n",
        "inputs,target = next(data_iter)\n",
        "position_embedding_layer = torch.nn.Embedding(max_length,256)\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape) #expected = 8,4,256\n",
        "pos_embeddings = position_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape) #expected = 4,256\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape) #expected = 8,4,256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5J1x4rOYRrC",
        "outputId": "a13a9f71-29e7-4c96-9755-534139fff6fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "torch.Size([4, 256])\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}